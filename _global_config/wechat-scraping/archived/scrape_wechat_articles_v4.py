#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ•°æ®æŠ“å–è„šæœ¬ v4.0
åŸºäº Playwright Codegen å½•åˆ¶çš„æ“ä½œæµç¨‹ä¼˜åŒ–
"""

from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
import json
import os
import time
from datetime import datetime

# é…ç½®
STORAGE_STATE_FILE = '/Users/dj/Desktop/å°é™çš„skills/_global_config/wechat_storage_state.json'
OUTPUT_FILE = '/Users/dj/Desktop/å°é™çš„skills/_global_config/wechat_articles_data.json'
SCREENSHOT_DIR = '/Users/dj/Desktop/å°é™çš„skills/_global_config/'

def scrape_articles():
    """æŠ“å–å¾®ä¿¡å…¬ä¼—å·å·²å‘è¡¨æ–‡ç« """
    print("=" * 60)
    print("å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ•°æ®æŠ“å– v4.0")
    print("åŸºäºå½•åˆ¶çš„æ“ä½œæµç¨‹")
    print("=" * 60)

    with sync_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        print("\nğŸš€ å¯åŠ¨æµè§ˆå™¨...")
        browser = p.chromium.launch(headless=False)

        # åŠ è½½ storage stateï¼ˆåŒ…å« cookiesï¼‰
        print("ğŸ”‘ åŠ è½½å·²ä¿å­˜çš„ç™»å½•çŠ¶æ€...")
        context = browser.new_context(storage_state=STORAGE_STATE_FILE)
        page = context.new_page()

        # è®¿é—®å·²å‘è¡¨é¡µé¢
        print("ğŸ“„ è®¿é—®å·²å‘è¡¨é¡µé¢...")
        page.goto("https://mp.weixin.qq.com/cgi-bin/appmsgpublish?sub=list")

        # ç­‰å¾…é¡µé¢åŠ è½½
        print("â³ ç­‰å¾…é¡µé¢åŠ è½½...")
        time.sleep(5)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
        try:
            login_link = page.get_by_role("link", name="ç™»å½•")
            if login_link.is_visible():
                print("âš ï¸  æ£€æµ‹åˆ°éœ€è¦ç™»å½•ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•...")
                print("â³ ç­‰å¾…ç™»å½•ï¼ˆ60ç§’ï¼‰...")
                time.sleep(60)
        except:
            print("âœ… å·²ç™»å½•")

        # å°è¯•å¯¼èˆªåˆ°å‘è¡¨è®°å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰
        try:
            print("\nğŸ” æŸ¥æ‰¾å‘è¡¨è®°å½•...")
            # ç‚¹å‡»"å†…å®¹ç®¡ç†"
            content_mgmt = page.get_by_title("å†…å®¹ç®¡ç†")
            if content_mgmt.is_visible():
                content_mgmt.click()
                time.sleep(1)

            # ç‚¹å‡»"å‘è¡¨è®°å½•"
            publish_record = page.get_by_role("link", name="å‘è¡¨è®°å½•")
            if publish_record.is_visible():
                publish_record.click()
                time.sleep(3)
        except Exception as e:
            print(f"âš ï¸  å¯¼èˆªæ“ä½œå¤±è´¥: {e}")
            print("ğŸ’¡ å¯èƒ½å·²ç»åœ¨æ­£ç¡®çš„é¡µé¢")

        # æˆªå›¾å½“å‰é¡µé¢
        screenshot_path = os.path.join(SCREENSHOT_DIR, 'articles_list_v4.png')
        page.screenshot(path=screenshot_path)
        print(f"ğŸ“¸ é¡µé¢æˆªå›¾å·²ä¿å­˜: {screenshot_path}")

        # æå–æ‰€æœ‰æ–‡ç« é“¾æ¥
        print("\nğŸ“Š æå–æ–‡ç« åˆ—è¡¨...")
        articles = []

        try:
            # è·å–æ‰€æœ‰æ–‡ç« é“¾æ¥
            # æ ¹æ®å½•åˆ¶çš„ä»£ç ï¼Œæ–‡ç« æ ‡é¢˜æ˜¯ role="link" çš„å…ƒç´ 
            article_links = page.get_by_role("link").all()

            print(f"âœ… æ‰¾åˆ° {len(article_links)} ä¸ªé“¾æ¥")

            # è¿‡æ»¤å‡ºæ–‡ç« é“¾æ¥ï¼ˆåŒ…å«"åŸåˆ›"æˆ–å…¶ä»–ç‰¹å¾ï¼‰
            for link in article_links:
                try:
                    text = link.inner_text()
                    href = link.get_attribute('href')

                    # è¿‡æ»¤æ¡ä»¶ï¼šæ–‡æœ¬ä¸ä¸ºç©ºï¼Œä¸”ä¸æ˜¯å¯¼èˆªé“¾æ¥
                    if text and len(text) > 5 and href:
                        # æ’é™¤å¯¼èˆªé“¾æ¥
                        if any(keyword in text for keyword in ['ç™»å½•', 'å†…å®¹ç®¡ç†', 'å‘è¡¨è®°å½•', 'ç´ æç®¡ç†']):
                            continue

                        articles.append({
                            'title': text,
                            'url': href,
                            'extracted_at': datetime.now().isoformat()
                        })
                        print(f"  - {text[:50]}...")
                except Exception as e:
                    continue

            print(f"\nâœ… æˆåŠŸæå– {len(articles)} ç¯‡æ–‡ç« ")

        except Exception as e:
            print(f"âŒ æå–å¤±è´¥: {e}")

            # ä¿å­˜é¡µé¢HTMLç”¨äºè°ƒè¯•
            html_path = os.path.join(SCREENSHOT_DIR, 'page_debug_v4.html')
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(page.content())
            print(f"ğŸ“ é¡µé¢HTMLå·²ä¿å­˜: {html_path}")

        # ä¿å­˜æ•°æ®
        if articles:
            with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
            print(f"\nâœ… æ–‡ç« æ•°æ®å·²ä¿å­˜: {OUTPUT_FILE}")

            # æ˜¾ç¤ºå‰3ç¯‡
            print("\nğŸ“ å‰3ç¯‡æ–‡ç« ï¼š")
            for i, article in enumerate(articles[:3], 1):
                print(f"\n{i}. {article['title']}")
                print(f"   URL: {article['url'][:80]}...")

        # ä¿æŒæµè§ˆå™¨æ‰“å¼€
        print("\nâ³ æµè§ˆå™¨å°†ä¿æŒæ‰“å¼€30ç§’ï¼Œè¯·æ£€æŸ¥ç»“æœ...")
        time.sleep(30)

        # å…³é—­
        context.close()
        browser.close()

    print("\n" + "=" * 60)
    print("âœ… æŠ“å–å®Œæˆï¼")
    print("=" * 60)

    return articles

def main():
    try:
        articles = scrape_articles()

        if articles:
            print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼š")
            print(f"  1. æŸ¥çœ‹ {OUTPUT_FILE} ä¸­çš„æ•°æ®")
            print(f"  2. æå–æ›´å¤šå­—æ®µï¼ˆå‘å¸ƒæ—¶é—´ã€åˆé›†ç­‰ï¼‰")
            print(f"  3. é›†æˆåˆ°é£ä¹¦è‡ªåŠ¨åŒæ­¥")
        else:
            print(f"\nâš ï¸  æœªæå–åˆ°æ–‡ç« æ•°æ®")
            print(f"  1. æŸ¥çœ‹æˆªå›¾ articles_list_v4.png")
            print(f"  2. æŸ¥çœ‹è°ƒè¯•HTML page_debug_v4.html")
            print(f"  3. æ‰‹åŠ¨æ£€æŸ¥é¡µé¢ç»“æ„")

    except Exception as e:
        print(f"\nâŒ æŠ“å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
