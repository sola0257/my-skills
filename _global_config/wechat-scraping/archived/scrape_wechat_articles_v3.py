#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ•°æ®æŠ“å–è„šæœ¬ v3.0
æ”¹è¿›ï¼šå¢åŠ é¡µé¢åŠ è½½ç­‰å¾…æ—¶é—´ï¼Œå¤„ç†åŠ¨æ€å†…å®¹
"""

from playwright.sync_api import sync_playwright
import json
import os
import time

# é…ç½®
COOKIES_FILE = '/Users/dj/Desktop/å°é™çš„skills/_global_config/wechat_cookies.json'
OUTPUT_FILE = '/Users/dj/Desktop/å°é™çš„skills/_global_config/wechat_articles_data.json'
SCREENSHOT_DIR = '/Users/dj/Desktop/å°é™çš„skills/_global_config/'

def load_cookies():
    """åŠ è½½ä¿å­˜çš„ cookies"""
    if os.path.exists(COOKIES_FILE):
        with open(COOKIES_FILE, 'r') as f:
            return json.load(f)
    return None

def save_cookies(context):
    """ä¿å­˜ cookies"""
    cookies = context.cookies()
    with open(COOKIES_FILE, 'w') as f:
        json.dump(cookies, f, indent=2)
    print(f"âœ… Cookies å·²ä¿å­˜: {COOKIES_FILE}")

def save_articles_data(articles):
    """ä¿å­˜æŠ“å–çš„æ–‡ç« æ•°æ®"""
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    print(f"âœ… æ–‡ç« æ•°æ®å·²ä¿å­˜: {OUTPUT_FILE}")

def wait_for_login(page):
    """ç­‰å¾…ç”¨æˆ·æ‰«ç ç™»å½•"""
    print("\nğŸ“± è¯·ä½¿ç”¨å¾®ä¿¡æ‰«æäºŒç»´ç ç™»å½•...")
    print("â³ ç­‰å¾…æ‰«ç ï¼ˆæœ€å¤š60ç§’ï¼‰...")

    # æˆªå›¾ä¿å­˜äºŒç»´ç 
    screenshot_path = os.path.join(SCREENSHOT_DIR, 'wechat_qrcode.png')
    page.screenshot(path=screenshot_path)
    print(f"ğŸ“¸ äºŒç»´ç å·²ä¿å­˜: {screenshot_path}")

    try:
        # ç­‰å¾…ç™»å½•æˆåŠŸ
        page.wait_for_url(lambda url: 'home' in url or 'cgi-bin' in url, timeout=60000)
        print("âœ… ç™»å½•æˆåŠŸï¼")
        # ç™»å½•æˆåŠŸåç­‰å¾…ä¸€ä¸‹ï¼Œç¡®ä¿ session ç¨³å®š
        page.wait_for_timeout(2000)
        return True
    except:
        print("âŒ ç™»å½•è¶…æ—¶ï¼Œè¯·é‡è¯•")
        return False

def scrape_articles():
    """æŠ“å–å¾®ä¿¡å…¬ä¼—å·å·²å‘è¡¨æ–‡ç« """
    print("=" * 60)
    print("å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ•°æ®æŠ“å– v3.0")
    print("=" * 60)

    with sync_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        print("\nğŸš€ å¯åŠ¨æµè§ˆå™¨...")
        browser = p.chromium.launch(
            headless=False,
            args=['--no-sandbox', '--disable-setuid-sandbox']
        )

        context = browser.new_context()
        page = context.new_page()

        # å°è¯•åŠ è½½ cookies
        cookies = load_cookies()
        need_login = True

        if cookies:
            print("ğŸ”‘ å°è¯•ä½¿ç”¨å·²ä¿å­˜çš„ cookies...")
            context.add_cookies(cookies)

            # å…ˆè®¿é—®é¦–é¡µï¼Œç¡®ä¿ cookies ç”Ÿæ•ˆ
            page.goto('https://mp.weixin.qq.com/')
            page.wait_for_timeout(3000)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
            if 'login' not in page.url.lower():
                print("âœ… Cookies æœ‰æ•ˆï¼Œå·²ç™»å½•")
                need_login = False
            else:
                print("âš ï¸  Cookies å·²è¿‡æœŸ")

        if need_login:
            print("\nğŸ“± éœ€è¦æ‰«ç ç™»å½•...")
            page.goto('https://mp.weixin.qq.com/')
            page.wait_for_timeout(2000)

            if not wait_for_login(page):
                browser.close()
                return None

            # ä¿å­˜æ–°çš„ cookies
            save_cookies(context)

        # ç°åœ¨è®¿é—®å·²å‘è¡¨é¡µé¢
        print("\nğŸ“„ è®¿é—®å·²å‘è¡¨é¡µé¢...")
        page.goto('https://mp.weixin.qq.com/cgi-bin/appmsgpublish?sub=list')

        # ç­‰å¾…é¡µé¢åŠ è½½ - å¢åŠ ç­‰å¾…æ—¶é—´
        print("â³ ç­‰å¾…é¡µé¢åŠ è½½ï¼ˆ10ç§’ï¼‰...")
        page.wait_for_timeout(10000)

        # æˆªå›¾å½“å‰é¡µé¢
        page.screenshot(path=os.path.join(SCREENSHOT_DIR, 'articles_page.png'))
        print("ğŸ“¸ é¡µé¢æˆªå›¾å·²ä¿å­˜: articles_page.png")

        # æ£€æŸ¥é¡µé¢å†…å®¹
        page_text = page.content()

        if 'è¯·é‡æ–°ç™»å½•' in page_text or 'è¯·ç™»å½•' in page_text:
            print("âŒ é¡µé¢è¦æ±‚é‡æ–°ç™»å½•ï¼Œsession å¯èƒ½å·²å¤±æ•ˆ")
            print("ğŸ’¡ å»ºè®®ï¼šå…³é—­æ‰€æœ‰æµè§ˆå™¨çª—å£ï¼Œé‡æ–°è¿è¡Œè„šæœ¬")
            browser.close()
            return None

        # ä¿å­˜å®Œæ•´çš„é¡µé¢HTMLç”¨äºåˆ†æ
        with open(os.path.join(SCREENSHOT_DIR, 'full_page.html'), 'w', encoding='utf-8') as f:
            f.write(page_text)
        print("ğŸ“ å®Œæ•´é¡µé¢HTMLå·²ä¿å­˜: full_page.html")

        print("\nğŸ” åˆ†æé¡µé¢å†…å®¹...")

        # å°è¯•æ‰§è¡Œ JavaScript è·å–é¡µé¢æ•°æ®
        articles_data = page.evaluate('''() => {
            // å°è¯•ä»å…¨å±€å˜é‡è·å–æ•°æ®
            if (window.wx && window.wx.data) {
                return {
                    success: true,
                    source: 'window.wx.data',
                    data: window.wx.data
                };
            }

            // å°è¯•ä» DOM è·å–
            const scripts = document.querySelectorAll('script');
            for (const script of scripts) {
                const text = script.textContent;
                if (text.includes('list') || text.includes('article')) {
                    return {
                        success: true,
                        source: 'script_tag',
                        snippet: text.substring(0, 500)
                    };
                }
            }

            // æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ–‡ç« å®¹å™¨
            const containers = document.querySelectorAll('[class*="list"], [class*="item"], [class*="card"]');

            return {
                success: containers.length > 0,
                source: 'dom_elements',
                count: containers.length,
                sample: containers.length > 0 ? containers[0].outerHTML.substring(0, 500) : null
            };
        }''')

        print(f"\nğŸ“Š é¡µé¢åˆ†æç»“æœï¼š")
        print(f"  æ•°æ®æ¥æº: {articles_data.get('source')}")
        print(f"  æˆåŠŸ: {articles_data.get('success')}")

        if articles_data.get('data'):
            print(f"  æ‰¾åˆ°å…¨å±€æ•°æ®å¯¹è±¡")
            print(f"  æ•°æ®å†…å®¹: {str(articles_data['data'])[:200]}...")

        if articles_data.get('snippet'):
            print(f"  æ‰¾åˆ°ç›¸å…³è„šæœ¬ç‰‡æ®µ")
            print(f"  ç‰‡æ®µå†…å®¹: {articles_data['snippet'][:200]}...")

        if articles_data.get('count'):
            print(f"  æ‰¾åˆ° {articles_data['count']} ä¸ªå¯èƒ½çš„å®¹å™¨å…ƒç´ ")

        # ä¿å­˜åˆ†æç»“æœ
        save_articles_data(articles_data)

        # ä¿æŒæµè§ˆå™¨æ‰“å¼€ï¼Œè®©ç”¨æˆ·å¯ä»¥æ‰‹åŠ¨æ£€æŸ¥
        print("\nâ³ æµè§ˆå™¨å°†ä¿æŒæ‰“å¼€60ç§’ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥é¡µé¢...")
        print("ğŸ’¡ ä½ å¯ä»¥ï¼š")
        print("  1. åœ¨æµè§ˆå™¨ä¸­å³é”®ç‚¹å‡»æ–‡ç« æ ‡é¢˜ â†’ æ£€æŸ¥å…ƒç´ ")
        print("  2. æŸ¥çœ‹å…ƒç´ çš„ class åç§°å’Œç»“æ„")
        print("  3. å‘Šè¯‰æˆ‘æ­£ç¡®çš„é€‰æ‹©å™¨")
        page.wait_for_timeout(60000)

        browser.close()

    print("\n" + "=" * 60)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("=" * 60)

    return articles_data

def main():
    try:
        result = scrape_articles()
        if result:
            print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼š")
            print(f"  1. æŸ¥çœ‹ full_page.html äº†è§£å®Œæ•´é¡µé¢ç»“æ„")
            print(f"  2. æŸ¥çœ‹ articles_page.png ç¡®è®¤é¡µé¢å†…å®¹")
            print(f"  3. åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨æ£€æŸ¥å…ƒç´ ï¼Œæ‰¾åˆ°æ­£ç¡®çš„é€‰æ‹©å™¨")
            print(f"  4. å‘Šè¯‰æˆ‘æ­£ç¡®çš„é€‰æ‹©å™¨ï¼Œæˆ‘æ¥æ›´æ–°è„šæœ¬")
    except Exception as e:
        print(f"\nâŒ æŠ“å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
