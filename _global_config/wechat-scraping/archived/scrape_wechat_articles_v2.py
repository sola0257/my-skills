#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ•°æ®æŠ“å–è„šæœ¬ v2.0
æ”¹è¿›ï¼šè‡ªåŠ¨æ£€æµ‹ç™»å½•çŠ¶æ€ï¼Œæ”¯æŒé‡æ–°ç™»å½•
"""

from playwright.sync_api import sync_playwright
import json
import os
from datetime import datetime

# é…ç½®
COOKIES_FILE = '/Users/dj/Desktop/å°é™çš„skills/_global_config/wechat_cookies.json'
OUTPUT_FILE = '/Users/dj/Desktop/å°é™çš„skills/_global_config/wechat_articles_data.json'
SCREENSHOT_DIR = '/Users/dj/Desktop/å°é™çš„skills/_global_config/'

def load_cookies():
    """åŠ è½½ä¿å­˜çš„ cookies"""
    if os.path.exists(COOKIES_FILE):
        with open(COOKIES_FILE, 'r') as f:
            return json.load(f)
    return None

def save_cookies(context):
    """ä¿å­˜ cookies"""
    cookies = context.cookies()
    with open(COOKIES_FILE, 'w') as f:
        json.dump(cookies, f, indent=2)
    print(f"âœ… Cookies å·²ä¿å­˜: {COOKIES_FILE}")

def save_articles_data(articles):
    """ä¿å­˜æŠ“å–çš„æ–‡ç« æ•°æ®"""
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    print(f"âœ… æ–‡ç« æ•°æ®å·²ä¿å­˜: {OUTPUT_FILE}")

def wait_for_login(page):
    """ç­‰å¾…ç”¨æˆ·æ‰«ç ç™»å½•"""
    print("\nğŸ“± è¯·ä½¿ç”¨å¾®ä¿¡æ‰«æäºŒç»´ç ç™»å½•...")
    print("â³ ç­‰å¾…æ‰«ç ï¼ˆæœ€å¤š60ç§’ï¼‰...")

    # æˆªå›¾ä¿å­˜äºŒç»´ç 
    screenshot_path = os.path.join(SCREENSHOT_DIR, 'wechat_qrcode.png')
    page.screenshot(path=screenshot_path)
    print(f"ğŸ“¸ äºŒç»´ç å·²ä¿å­˜: {screenshot_path}")

    try:
        # ç­‰å¾…ç™»å½•æˆåŠŸï¼ˆURL å˜åŒ–ï¼‰
        page.wait_for_url(lambda url: 'home' in url or 'cgi-bin' in url, timeout=60000)
        print("âœ… ç™»å½•æˆåŠŸï¼")
        return True
    except:
        print("âŒ ç™»å½•è¶…æ—¶ï¼Œè¯·é‡è¯•")
        return False

def check_login_status(page):
    """æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°ç™»å½•"""
    current_url = page.url

    # æ£€æŸ¥æ˜¯å¦åœ¨ç™»å½•é¡µé¢
    if 'login' in current_url.lower():
        return False

    # æ£€æŸ¥é¡µé¢å†…å®¹
    try:
        # æŸ¥æ‰¾"è¯·é‡æ–°ç™»å½•"æ–‡æœ¬
        relogin_text = page.locator('text=è¯·é‡æ–°ç™»å½•').count()
        if relogin_text > 0:
            return False
    except:
        pass

    return True

def scrape_articles():
    """æŠ“å–å¾®ä¿¡å…¬ä¼—å·å·²å‘è¡¨æ–‡ç« """
    print("=" * 60)
    print("å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ•°æ®æŠ“å– v2.0")
    print("=" * 60)

    with sync_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        print("\nğŸš€ å¯åŠ¨æµè§ˆå™¨...")
        browser = p.chromium.launch(
            headless=False,
            args=['--no-sandbox', '--disable-setuid-sandbox']
        )

        context = browser.new_context()
        page = context.new_page()

        # å°è¯•åŠ è½½ cookies
        cookies = load_cookies()
        if cookies:
            print("ğŸ”‘ åŠ è½½å·²ä¿å­˜çš„ cookies...")
            context.add_cookies(cookies)

        # è®¿é—®å·²å‘è¡¨é¡µé¢
        print("ğŸ“„ è®¿é—®å·²å‘è¡¨é¡µé¢...")
        page.goto('https://mp.weixin.qq.com/cgi-bin/appmsgpublish?sub=list')
        page.wait_for_timeout(3000)

        # æ£€æŸ¥ç™»å½•çŠ¶æ€
        if not check_login_status(page):
            print("âš ï¸  éœ€è¦é‡æ–°ç™»å½•")

            # å¦‚æœä¸åœ¨ç™»å½•é¡µé¢ï¼Œå…ˆè·³è½¬åˆ°ç™»å½•é¡µ
            if 'login' not in page.url.lower():
                page.goto('https://mp.weixin.qq.com/')
                page.wait_for_timeout(2000)

            # ç­‰å¾…ç”¨æˆ·æ‰«ç ç™»å½•
            if not wait_for_login(page):
                browser.close()
                return None

            # ä¿å­˜æ–°çš„ cookies
            save_cookies(context)

            # é‡æ–°è®¿é—®å·²å‘è¡¨é¡µé¢
            print("\nğŸ“„ é‡æ–°è®¿é—®å·²å‘è¡¨é¡µé¢...")
            page.goto('https://mp.weixin.qq.com/cgi-bin/appmsgpublish?sub=list')
            page.wait_for_timeout(3000)
        else:
            print("âœ… ç™»å½•çŠ¶æ€æœ‰æ•ˆ")

        print("\nğŸ” å¼€å§‹åˆ†æé¡µé¢ç»“æ„...")

        # å…ˆè·å–é¡µé¢çš„ HTML ç»“æ„ï¼Œå¸®åŠ©æˆ‘ä»¬æ‰¾åˆ°æ­£ç¡®çš„é€‰æ‹©å™¨
        page_content = page.content()

        # ä¿å­˜ HTML ç”¨äºè°ƒè¯•
        with open(os.path.join(SCREENSHOT_DIR, 'page_structure.html'), 'w', encoding='utf-8') as f:
            f.write(page_content)
        print("ğŸ“ é¡µé¢ç»“æ„å·²ä¿å­˜: page_structure.html")

        # æˆªå›¾å½“å‰é¡µé¢
        page.screenshot(path=os.path.join(SCREENSHOT_DIR, 'current_page.png'))
        print("ğŸ“¸ é¡µé¢æˆªå›¾å·²ä¿å­˜: current_page.png")

        # å°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
        print("\nğŸ” å°è¯•æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨...")

        # è·å–æ‰€æœ‰å¯èƒ½çš„æ–‡ç« å®¹å™¨
        articles_data = page.evaluate('''() => {
            // å°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
            const selectors = [
                '.appmsg-list-item',
                '.appmsg_item',
                '.publish_card',
                '[class*="publish"]',
                '[class*="article"]',
                '[class*="msg"]'
            ];

            let items = [];
            for (const selector of selectors) {
                const elements = document.querySelectorAll(selector);
                if (elements.length > 0) {
                    console.log(`æ‰¾åˆ° ${elements.length} ä¸ªå…ƒç´ ï¼Œé€‰æ‹©å™¨: ${selector}`);
                    items = Array.from(elements);
                    break;
                }
            }

            if (items.length === 0) {
                return {
                    success: false,
                    message: 'æœªæ‰¾åˆ°æ–‡ç« åˆ—è¡¨',
                    html: document.body.innerHTML.substring(0, 1000)
                };
            }

            // æå–æ–‡ç« ä¿¡æ¯
            const articles = items.map(item => {
                return {
                    html: item.innerHTML.substring(0, 500),
                    text: item.textContent.trim().substring(0, 200)
                };
            });

            return {
                success: true,
                count: articles.length,
                articles: articles
            };
        }''')

        if not articles_data['success']:
            print(f"âŒ {articles_data['message']}")
            print("\nğŸ’¡ å»ºè®®ï¼š")
            print("  1. æŸ¥çœ‹ page_structure.html äº†è§£é¡µé¢ç»“æ„")
            print("  2. æŸ¥çœ‹ current_page.png ç¡®è®¤é¡µé¢å†…å®¹")
            print("  3. æ‰‹åŠ¨åœ¨æµè§ˆå™¨ä¸­æ£€æŸ¥å…ƒç´ ï¼Œæ‰¾åˆ°æ­£ç¡®çš„é€‰æ‹©å™¨")

            # ä¿æŒæµè§ˆå™¨æ‰“å¼€ï¼Œè®©ç”¨æˆ·å¯ä»¥æ‰‹åŠ¨æ£€æŸ¥
            print("\nâ³ æµè§ˆå™¨å°†ä¿æŒæ‰“å¼€30ç§’ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥é¡µé¢...")
            page.wait_for_timeout(30000)
        else:
            print(f"âœ… æ‰¾åˆ° {articles_data['count']} ç¯‡æ–‡ç« ")
            print("\nğŸ“ å‰3ç¯‡æ–‡ç« çš„HTMLç‰‡æ®µï¼š")
            for i, article in enumerate(articles_data['articles'][:3], 1):
                print(f"\n--- æ–‡ç«  {i} ---")
                print(f"æ–‡æœ¬å†…å®¹: {article['text'][:100]}...")

            # ä¿å­˜åŸå§‹æ•°æ®
            save_articles_data(articles_data)

            print("\nâ³ æµè§ˆå™¨å°†ä¿æŒæ‰“å¼€30ç§’ï¼Œè¯·æŸ¥çœ‹ç»“æœ...")
            page.wait_for_timeout(30000)

        browser.close()

    print("\n" + "=" * 60)
    print("âœ… æŠ“å–å®Œæˆï¼")
    print("=" * 60)

    return articles_data

def main():
    try:
        result = scrape_articles()
        if result and result.get('success'):
            print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼š")
            print(f"  1. æŸ¥çœ‹ page_structure.html äº†è§£é¡µé¢ç»“æ„")
            print(f"  2. æ ¹æ®å®é™…ç»“æ„ç¼–å†™ç²¾ç¡®çš„æ•°æ®æå–é€»è¾‘")
            print(f"  3. æå–æ ‡é¢˜ã€å‘å¸ƒæ—¶é—´ã€åˆé›†ç­‰å­—æ®µ")
            print(f"  4. é›†æˆåˆ°é£ä¹¦è‡ªåŠ¨åŒæ­¥")
    except Exception as e:
        print(f"\nâŒ æŠ“å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
