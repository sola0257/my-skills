#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ•°æ®æŠ“å–è„šæœ¬
åŠŸèƒ½ï¼šä»å·²å‘è¡¨é¡µé¢æŠ“å–æ–‡ç« åˆ—è¡¨ï¼Œæå–æ ‡é¢˜ã€å‘å¸ƒæ—¶é—´ã€å†…å®¹ç±»å‹ã€åˆé›†ç­‰ä¿¡æ¯
"""

from playwright.sync_api import sync_playwright
import json
import os
from datetime import datetime

# é…ç½®
COOKIES_FILE = '/Users/dj/Desktop/å°é™çš„skills/_global_config/wechat_cookies.json'
OUTPUT_FILE = '/Users/dj/Desktop/å°é™çš„skills/_global_config/wechat_articles_data.json'

def load_cookies():
    """åŠ è½½ä¿å­˜çš„ cookies"""
    if os.path.exists(COOKIES_FILE):
        with open(COOKIES_FILE, 'r') as f:
            return json.load(f)
    return None

def save_articles_data(articles):
    """ä¿å­˜æŠ“å–çš„æ–‡ç« æ•°æ®"""
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    print(f"âœ… æ–‡ç« æ•°æ®å·²ä¿å­˜: {OUTPUT_FILE}")

def scrape_articles():
    """æŠ“å–å¾®ä¿¡å…¬ä¼—å·å·²å‘è¡¨æ–‡ç« """
    print("=" * 60)
    print("å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ•°æ®æŠ“å–")
    print("=" * 60)

    # åŠ è½½ cookies
    cookies = load_cookies()
    if not cookies:
        print("âŒ æœªæ‰¾åˆ° cookies æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ test_playwright_login.py ç™»å½•")
        return

    with sync_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        print("\nğŸš€ å¯åŠ¨æµè§ˆå™¨...")
        browser = p.chromium.launch(
            headless=False,
            args=['--no-sandbox', '--disable-setuid-sandbox']
        )

        context = browser.new_context()
        page = context.new_page()

        # è®¾ç½® cookies
        print("ğŸ”‘ åŠ è½½ cookies...")
        context.add_cookies(cookies)

        # è®¿é—®å·²å‘è¡¨é¡µé¢
        print("ğŸ“„ è®¿é—®å·²å‘è¡¨é¡µé¢...")
        page.goto('https://mp.weixin.qq.com/cgi-bin/appmsgpublish?sub=list')
        page.wait_for_timeout(3000)

        # æ£€æŸ¥æ˜¯å¦ç™»å½•æˆåŠŸ
        current_url = page.url
        if 'login' in current_url.lower():
            print("âŒ Cookies å·²è¿‡æœŸï¼Œè¯·é‡æ–°è¿è¡Œ test_playwright_login.py ç™»å½•")
            browser.close()
            return

        print("âœ… ç™»å½•æˆåŠŸï¼Œå¼€å§‹æŠ“å–æ•°æ®...")

        # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
        try:
            # ç­‰å¾…æ–‡ç« åˆ—è¡¨åŠ è½½
            page.wait_for_selector('.appmsg-list', timeout=10000)
            print("âœ… æ–‡ç« åˆ—è¡¨åŠ è½½å®Œæˆ")
        except:
            print("âš ï¸  æœªæ‰¾åˆ°æ–‡ç« åˆ—è¡¨ï¼Œå¯èƒ½é¡µé¢ç»“æ„å·²å˜åŒ–")
            # æˆªå›¾ä¿å­˜å½“å‰é¡µé¢
            page.screenshot(path='/Users/dj/Desktop/å°é™çš„skills/_global_config/debug_page.png')
            print("ğŸ“¸ å·²ä¿å­˜è°ƒè¯•æˆªå›¾: debug_page.png")
            browser.close()
            return

        # æå–æ–‡ç« åˆ—è¡¨
        print("\nğŸ“Š æå–æ–‡ç« æ•°æ®...")
        articles = page.evaluate('''() => {
            const items = document.querySelectorAll('.appmsg-list-item');
            return Array.from(items).map(item => {
                // æå–æ ‡é¢˜
                const titleElem = item.querySelector('.title');
                const title = titleElem ? titleElem.textContent.trim() : '';

                // æå–å‘å¸ƒæ—¶é—´
                const timeElem = item.querySelector('.time');
                const publishTime = timeElem ? timeElem.textContent.trim() : '';

                // æå–æ–‡ç« é“¾æ¥
                const linkElem = item.querySelector('a');
                const url = linkElem ? linkElem.href : '';

                // æå–å†…å®¹ç±»å‹ï¼ˆå›¾æ–‡/é•¿æ–‡ï¼‰
                // è¿™ä¸ªéœ€è¦æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´
                const typeElem = item.querySelector('.type');
                const contentType = typeElem ? typeElem.textContent.trim() : 'æœªçŸ¥';

                return {
                    title: title,
                    publishTime: publishTime,
                    url: url,
                    contentType: contentType
                };
            });
        }''')

        print(f"âœ… æˆåŠŸæå– {len(articles)} ç¯‡æ–‡ç« ")

        # æ‰“å°å‰3ç¯‡æ–‡ç« ä½œä¸ºç¤ºä¾‹
        if articles:
            print("\nğŸ“ ç¤ºä¾‹æ•°æ®ï¼ˆå‰3ç¯‡ï¼‰ï¼š")
            for i, article in enumerate(articles[:3], 1):
                print(f"\n{i}. {article['title']}")
                print(f"   å‘å¸ƒæ—¶é—´: {article['publishTime']}")
                print(f"   å†…å®¹ç±»å‹: {article['contentType']}")

        # ä¿å­˜æ•°æ®
        save_articles_data(articles)

        # ä¿æŒæµè§ˆå™¨æ‰“å¼€ä¸€æ®µæ—¶é—´
        print("\nâ³ æµè§ˆå™¨å°†åœ¨5ç§’åå…³é—­...")
        page.wait_for_timeout(5000)

        browser.close()

    print("\n" + "=" * 60)
    print("âœ… æŠ“å–å®Œæˆï¼")
    print("=" * 60)

    return articles

def main():
    try:
        articles = scrape_articles()
        if articles:
            print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼š")
            print(f"  1. æ£€æŸ¥ {OUTPUT_FILE} ä¸­çš„æ•°æ®")
            print(f"  2. æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´é€‰æ‹©å™¨")
            print(f"  3. å®ç°åˆé›†ä¿¡æ¯æŠ“å–")
            print(f"  4. é›†æˆåˆ°é£ä¹¦è‡ªåŠ¨åŒæ­¥")
    except Exception as e:
        print(f"\nâŒ æŠ“å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
