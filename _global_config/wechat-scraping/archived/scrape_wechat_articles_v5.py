#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ•°æ®æŠ“å–è„šæœ¬ v5.0
åŸºäºå®Œæ•´å½•åˆ¶æµç¨‹ä¼˜åŒ–
åŠŸèƒ½ï¼š
1. è‡ªåŠ¨ç™»å½•ï¼ˆæ‰«ç ï¼‰
2. æŠ“å–æ‰€æœ‰å·²å‘è¡¨æ–‡ç« 
3. æ”¯æŒç¿»é¡µ
4. æå–æ ‡é¢˜ã€URLã€å‘å¸ƒæ—¶é—´ç­‰ä¿¡æ¯
"""

from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
import json
import os
import time
from datetime import datetime
import re

# é…ç½®
OUTPUT_FILE = '/Users/dj/Desktop/å°é™çš„skills/_global_config/wechat_articles_data.json'
SCREENSHOT_DIR = '/Users/dj/Desktop/å°é™çš„skills/_global_config/'

def wait_for_login(page):
    """ç­‰å¾…ç”¨æˆ·æ‰«ç ç™»å½•"""
    print("\nğŸ“± è¯·ä½¿ç”¨å¾®ä¿¡æ‰«æäºŒç»´ç ç™»å½•...")
    print("â³ ç­‰å¾…æ‰«ç ï¼ˆæœ€å¤š60ç§’ï¼‰...")

    # æˆªå›¾ä¿å­˜äºŒç»´ç 
    screenshot_path = os.path.join(SCREENSHOT_DIR, 'qrcode_login.png')
    page.screenshot(path=screenshot_path)
    print(f"ğŸ“¸ äºŒç»´ç å·²ä¿å­˜: {screenshot_path}")

    try:
        # ç­‰å¾…ç™»å½•æˆåŠŸï¼ˆURL å˜åŒ–åˆ°é¦–é¡µï¼‰
        page.wait_for_url(lambda url: 'home' in url or 'cgi-bin' in url, timeout=60000)
        print("âœ… ç™»å½•æˆåŠŸï¼")

        # ä¿å­˜ç™»å½•åçš„ cookies
        context = page.context
        cookies = context.cookies()
        cookies_file = os.path.join(SCREENSHOT_DIR, 'wechat_cookies.json')
        with open(cookies_file, 'w') as f:
            json.dump(cookies, f, indent=2)
        print(f"âœ… Cookies å·²ä¿å­˜: {cookies_file}")

        return True
    except:
        print("âŒ ç™»å½•è¶…æ—¶ï¼Œè¯·é‡è¯•")
        return False

def navigate_to_published_list(page):
    """å¯¼èˆªåˆ°å·²å‘è¡¨é¡µé¢"""
    print("\nğŸ” å¯¼èˆªåˆ°å·²å‘è¡¨é¡µé¢...")

    try:
        # ç‚¹å‡»"å†…å®¹ç®¡ç†"
        page.get_by_text("å†…å®¹ç®¡ç†").click()
        time.sleep(1)

        # ç‚¹å‡»"å‘è¡¨è®°å½•"
        page.get_by_role("link", name="å‘è¡¨è®°å½•").click()
        time.sleep(3)

        print("âœ… å·²è¿›å…¥å‘è¡¨è®°å½•é¡µé¢")
        return True
    except Exception as e:
        print(f"âŒ å¯¼èˆªå¤±è´¥: {e}")
        return False

def extract_articles_from_page(page):
    """ä»å½“å‰é¡µé¢æå–æ–‡ç« åˆ—è¡¨"""
    articles = []

    try:
        # è·å–æ‰€æœ‰æ–‡ç« é“¾æ¥
        article_links = page.get_by_role("link").all()

        for link in article_links:
            try:
                text = link.inner_text()
                href = link.get_attribute('href')

                # è¿‡æ»¤æ¡ä»¶ï¼šæ’é™¤å¯¼èˆªé“¾æ¥
                if not text or len(text) < 5:
                    continue

                # æ’é™¤å¯¼èˆªå’ŒåŠŸèƒ½é“¾æ¥
                exclude_keywords = ['ç™»å½•', 'å†…å®¹ç®¡ç†', 'å‘è¡¨è®°å½•', 'ç´ æç®¡ç†',
                                   'åŸåˆ›', 'åˆé›†', 'ä¸‹ä¸€é¡µ', 'ä¸Šä¸€é¡µ', 'é¦–é¡µ', 'å°¾é¡µ']
                if any(keyword in text for keyword in exclude_keywords):
                    continue

                # åªä¿ç•™æ–‡ç« é“¾æ¥ï¼ˆåŒ…å« appmsg æˆ– s?__bizï¼‰
                if href and ('appmsg' in href or 's?__biz' in href):
                    articles.append({
                        'title': text.strip(),
                        'url': href,
                        'extracted_at': datetime.now().isoformat()
                    })

            except Exception as e:
                continue

    except Exception as e:
        print(f"âš ï¸  æå–æ–‡ç« å¤±è´¥: {e}")

    return articles

def scrape_all_articles():
    """æŠ“å–æ‰€æœ‰æ–‡ç« ï¼ˆæ”¯æŒç¿»é¡µï¼‰"""
    print("=" * 60)
    print("å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ•°æ®æŠ“å– v5.0")
    print("=" * 60)

    all_articles = []

    with sync_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        print("\nğŸš€ å¯åŠ¨æµè§ˆå™¨...")
        browser = p.chromium.launch(headless=False)
        context = browser.new_context()
        page = context.new_page()

        # è®¿é—®ç™»å½•é¡µ
        print("ğŸ“„ è®¿é—®å¾®ä¿¡å…¬ä¼—å·...")
        page.goto('https://mp.weixin.qq.com/')
        time.sleep(2)

        # ç­‰å¾…ç™»å½•
        if not wait_for_login(page):
            browser.close()
            return None

        # å¯¼èˆªåˆ°å·²å‘è¡¨é¡µé¢
        if not navigate_to_published_list(page):
            browser.close()
            return None

        # æŠ“å–ç¬¬ä¸€é¡µ
        print("\nğŸ“Š å¼€å§‹æŠ“å–æ–‡ç« ...")
        page_num = 1
        
        while True:
            print(f"\nğŸ“„ æ­£åœ¨æŠ“å–ç¬¬ {page_num} é¡µ...")
            
            # æå–å½“å‰é¡µæ–‡ç« 
            articles = extract_articles_from_page(page)
            
            if articles:
                print(f"  âœ… æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
                all_articles.extend(articles)
                
                # æ˜¾ç¤ºå‰3ç¯‡
                for i, article in enumerate(articles[:3], 1):
                    print(f"    {i}. {article['title'][:50]}...")
            else:
                print("  âš ï¸  æœ¬é¡µæœªæ‰¾åˆ°æ–‡ç« ")

            # å°è¯•ç¿»é¡µ
            try:
                next_button = page.get_by_role("link", name="ä¸‹ä¸€é¡µ")
                if next_button.is_visible() and next_button.is_enabled():
                    print("  â­ï¸  ç¿»åˆ°ä¸‹ä¸€é¡µ...")
                    next_button.click()
                    time.sleep(3)
                    page_num += 1
                else:
                    print("  âœ… å·²åˆ°æœ€åä¸€é¡µ")
                    break
            except:
                print("  âœ… å·²åˆ°æœ€åä¸€é¡µ")
                break

        print(f"\nâœ… æŠ“å–å®Œæˆï¼å…± {len(all_articles)} ç¯‡æ–‡ç« ")

        # ä¿å­˜æ•°æ®
        if all_articles:
            with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                json.dump(all_articles, f, ensure_ascii=False, indent=2)
            print(f"âœ… æ•°æ®å·²ä¿å­˜: {OUTPUT_FILE}")

        # æˆªå›¾
        screenshot_path = os.path.join(SCREENSHOT_DIR, 'final_page.png')
        page.screenshot(path=screenshot_path)
        print(f"ğŸ“¸ æœ€ç»ˆé¡µé¢æˆªå›¾: {screenshot_path}")

        # ä¿æŒæµè§ˆå™¨æ‰“å¼€
        print("\nâ³ æµè§ˆå™¨å°†ä¿æŒæ‰“å¼€30ç§’...")
        time.sleep(30)

        browser.close()

    return all_articles

def main():
    try:
        articles = scrape_all_articles()
        
        if articles:
            print(f"\n" + "=" * 60)
            print(f"âœ… æˆåŠŸæŠ“å– {len(articles)} ç¯‡æ–‡ç« ")
            print("=" * 60)
            
            print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼š")
            print(f"  1. æŸ¥çœ‹ {OUTPUT_FILE}")
            print(f"  2. æå–æ›´å¤šå­—æ®µï¼ˆå‘å¸ƒæ—¶é—´ã€åˆé›†ç­‰ï¼‰")
            print(f"  3. é›†æˆåˆ°é£ä¹¦è‡ªåŠ¨åŒæ­¥")
        else:
            print("\nâš ï¸  æœªæŠ“å–åˆ°æ–‡ç« ")
            
    except Exception as e:
        print(f"\nâŒ æŠ“å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
