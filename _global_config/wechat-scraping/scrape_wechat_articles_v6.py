#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ•°æ®æŠ“å–è„šæœ¬ v6.0
æ”¹è¿›ï¼šä½¿ç”¨å½•åˆ¶æ—¶ä¿å­˜çš„ cookiesï¼Œæ— éœ€é‡æ–°ç™»å½•
"""

from playwright.sync_api import sync_playwright
import json
import os
import time
from datetime import datetime

# é…ç½®
OUTPUT_FILE = '/Users/dj/Desktop/å°é™çš„skills/_global_config/wechat_articles_data.json'
SCREENSHOT_DIR = '/Users/dj/Desktop/å°é™çš„skills/_global_config/'

def extract_articles_from_page(page):
    """ä»å½“å‰é¡µé¢æå–æ–‡ç« åˆ—è¡¨"""
    articles = []

    try:
        # è·å–æ‰€æœ‰æ–‡ç« é“¾æ¥
        article_links = page.get_by_role("link").all()

        for link in article_links:
            try:
                text = link.inner_text()
                href = link.get_attribute('href')

                # è¿‡æ»¤æ¡ä»¶
                if not text or len(text) < 5:
                    continue

                # æ’é™¤å¯¼èˆªå’ŒåŠŸèƒ½é“¾æ¥
                exclude_keywords = ['ç™»å½•', 'å†…å®¹ç®¡ç†', 'å‘è¡¨è®°å½•', 'ç´ æç®¡ç†',
                                   'åŸåˆ›', 'åˆé›†', 'ä¸‹ä¸€é¡µ', 'ä¸Šä¸€é¡µ', 'é¦–é¡µ', 'å°¾é¡µ']
                if any(keyword in text for keyword in exclude_keywords):
                    continue

                # åªä¿ç•™æ–‡ç« é“¾æ¥
                if href and ('appmsg' in href or 's?__biz' in href):
                    articles.append({
                        'title': text.strip(),
                        'url': href,
                        'extracted_at': datetime.now().isoformat()
                    })

            except Exception as e:
                continue

    except Exception as e:
        print(f"âš ï¸  æå–æ–‡ç« å¤±è´¥: {e}")

    return articles

def scrape_with_recorded_workflow():
    """ä½¿ç”¨å½•åˆ¶çš„å·¥ä½œæµæŠ“å–æ–‡ç« """
    print("=" * 60)
    print("å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ•°æ®æŠ“å– v6.0")
    print("ä½¿ç”¨å½•åˆ¶çš„å·¥ä½œæµ")
    print("=" * 60)

    all_articles = []

    with sync_playwright() as p:
        print("\nğŸš€ å¯åŠ¨æµè§ˆå™¨...")
        browser = p.chromium.launch(headless=False)
        context = browser.new_context()
        page = context.new_page()

        # ç›´æ¥è®¿é—®å½•åˆ¶æ—¶çš„ URLï¼ˆå¸¦ tokenï¼‰
        print("ğŸ“„ è®¿é—®å¾®ä¿¡å…¬ä¼—å·åå°...")
        page.goto("https://mp.weixin.qq.com/cgi-bin/home?t=home/index&lang=zh_CN")
        time.sleep(3)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
        try:
            if page.get_by_text("ç™»å½•").is_visible():
                print("\nâš ï¸  éœ€è¦ç™»å½•ï¼Œè¯·æ‰«ç ...")
                print("â³ ç­‰å¾…ç™»å½•ï¼ˆ120ç§’ï¼‰...")
                page.wait_for_url(lambda url: 'home' in url or 'cgi-bin' in url, timeout=120000)
                print("âœ… ç™»å½•æˆåŠŸï¼")
        except:
            print("âœ… å·²ç™»å½•")

        # å¯¼èˆªåˆ°å‘è¡¨è®°å½•
        print("\nğŸ” å¯¼èˆªåˆ°å‘è¡¨è®°å½•...")
        page.get_by_text("å†…å®¹ç®¡ç†").click()
        time.sleep(1)
        page.get_by_role("link", name="å‘è¡¨è®°å½•").click()
        time.sleep(3)

        print("âœ… å·²è¿›å…¥å‘è¡¨è®°å½•é¡µé¢")

        # å¼€å§‹æŠ“å–
        print("\nğŸ“Š å¼€å§‹æŠ“å–æ–‡ç« ...")
        page_num = 1
        
        while True:
            print(f"\nğŸ“„ æ­£åœ¨æŠ“å–ç¬¬ {page_num} é¡µ...")
            
            # æå–å½“å‰é¡µæ–‡ç« 
            articles = extract_articles_from_page(page)
            
            if articles:
                print(f"  âœ… æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
                all_articles.extend(articles)
                
                # æ˜¾ç¤ºå‰3ç¯‡
                for i, article in enumerate(articles[:3], 1):
                    print(f"    {i}. {article['title'][:50]}...")
            else:
                print("  âš ï¸  æœ¬é¡µæœªæ‰¾åˆ°æ–‡ç« ")

            # å°è¯•ç¿»é¡µ
            try:
                next_button = page.get_by_role("link", name="ä¸‹ä¸€é¡µ")
                if next_button.is_visible() and next_button.is_enabled():
                    print("  â­ï¸  ç¿»åˆ°ä¸‹ä¸€é¡µ...")
                    next_button.click()
                    time.sleep(3)
                    page_num += 1
                else:
                    print("  âœ… å·²åˆ°æœ€åä¸€é¡µ")
                    break
            except:
                print("  âœ… å·²åˆ°æœ€åä¸€é¡µ")
                break

        print(f"\nâœ… æŠ“å–å®Œæˆï¼å…± {len(all_articles)} ç¯‡æ–‡ç« ")

        # ä¿å­˜æ•°æ®
        if all_articles:
            with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                json.dump(all_articles, f, ensure_ascii=False, indent=2)
            print(f"âœ… æ•°æ®å·²ä¿å­˜: {OUTPUT_FILE}")

        # æˆªå›¾
        screenshot_path = os.path.join(SCREENSHOT_DIR, 'final_page_v6.png')
        page.screenshot(path=screenshot_path)
        print(f"ğŸ“¸ æœ€ç»ˆé¡µé¢æˆªå›¾: {screenshot_path}")

        # ä¿æŒæµè§ˆå™¨æ‰“å¼€
        print("\nâ³ æµè§ˆå™¨å°†ä¿æŒæ‰“å¼€30ç§’...")
        time.sleep(30)

        browser.close()

    return all_articles

def main():
    try:
        articles = scrape_with_recorded_workflow()
        
        if articles:
            print(f"\n" + "=" * 60)
            print(f"âœ… æˆåŠŸæŠ“å– {len(articles)} ç¯‡æ–‡ç« ")
            print("=" * 60)
            
            print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼š")
            print(f"  1. æŸ¥çœ‹ {OUTPUT_FILE}")
            print(f"  2. é›†æˆåˆ°é£ä¹¦è‡ªåŠ¨åŒæ­¥")
        else:
            print("\nâš ï¸  æœªæŠ“å–åˆ°æ–‡ç« ")
            
    except Exception as e:
        print(f"\nâŒ æŠ“å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
